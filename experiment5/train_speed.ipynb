{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:18.862574Z",
     "iopub.status.busy": "2021-01-16T03:00:18.862161Z",
     "iopub.status.idle": "2021-01-16T03:00:21.515540Z",
     "shell.execute_reply": "2021-01-16T03:00:21.514700Z",
     "shell.execute_reply.started": "2021-01-16T03:00:18.862526Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import optimizers,Sequential, layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:21.517337Z",
     "iopub.status.busy": "2021-01-16T03:00:21.517135Z",
     "iopub.status.idle": "2021-01-16T03:00:21.536402Z",
     "shell.execute_reply": "2021-01-16T03:00:21.535747Z",
     "shell.execute_reply.started": "2021-01-16T03:00:21.517311Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一维卷积后，再全连接获得三层预测\n",
    "class FcCvModelReFCSpeed(Model):\n",
    "    def __init__(self):\n",
    "        super(FcCvModelReFCSpeed, self).__init__()\n",
    "        # 多层全链接的第一层，每一道数据的提取\n",
    "        # 实际上是在每道上的滑动全连接\n",
    "        self.fcOne = Sequential([\n",
    "            layers.Dense(64),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dense(8),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        # 1.减少过拟合的情况\n",
    "        # 2.模拟道数据的丢失\n",
    "        self.dropout_fc = layers.Dropout(0.3)\n",
    "\n",
    "        # 1维卷积综合提取特征\n",
    "        self.conv1 = Sequential([\n",
    "            layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c1 = layers.Dropout(0.3)\n",
    "        self.conv2 = Sequential([\n",
    "            layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c2 = layers.Dropout(0.3)\n",
    "        # 1维卷积综合提取特征+降维\n",
    "        self.conv3 = Sequential([\n",
    "            layers.Conv1D(filters=128, kernel_size=4, strides=4, padding='valid'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c3 = layers.Dropout(0.3)\n",
    "        # 1维卷积综合提取特征\n",
    "        self.conv4 = Sequential([\n",
    "            layers.Conv1D(filters=128, kernel_size=7, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c4 = layers.Dropout(0.3)\n",
    "        self.conv5 = Sequential([\n",
    "            layers.Conv1D(filters=128, kernel_size=7, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c5 = layers.Dropout(0.3)\n",
    "        # 1维卷积综合提取特征+降维\n",
    "        self.conv6 = Sequential([\n",
    "            layers.Conv1D(filters=256, kernel_size=4, strides=4, padding='valid'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        \n",
    "        # 最终作出的预测\n",
    "        # 速度预测\n",
    "        self.fcThree_s1 = Sequential([\n",
    "            layers.Dense(8),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('softmax')\n",
    "        ])\n",
    "        self.fcThree_s2 = Sequential([\n",
    "            layers.Dense(8),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('softmax')\n",
    "        ])\n",
    "        self.fcThree_s3 = Sequential([\n",
    "            layers.Dense(8),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('softmax')\n",
    "        ])\n",
    "        self.fcThree_s4 = Sequential([\n",
    "            layers.Dense(8),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        batchSize, dao_num, data_num = inputs.shape\n",
    "        x = tf.reshape(inputs, (-1, 1024))\n",
    "        x = self.fcOne(x)\n",
    "        x = tf.reshape(x, (batchSize, dao_num, -1))\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])\n",
    "        x = self.dropout_fc(x, training=training)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout_c1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout_c2(x, training=training)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout_c3(x, training=training)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout_c4(x, training=training)\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout_c5(x, training=training)\n",
    "        x = self.conv6(x)\n",
    "        x = tf.reshape(x, (batchSize, -1))\n",
    "        x_s1 = self.fcThree_s1(x)\n",
    "        x_s2 = self.fcThree_s2(x)\n",
    "        x_s3 = self.fcThree_s3(x)\n",
    "        x_s4 = self.fcThree_s4(x)\n",
    "        x_s = tf.concat([tf.expand_dims(x_s1,axis=1),tf.expand_dims(x_s2,axis=1),tf.expand_dims(x_s3,axis=1),tf.expand_dims(x_s4,axis=1)],axis=1)\n",
    "        return x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:21.537918Z",
     "iopub.status.busy": "2021-01-16T03:00:21.537728Z",
     "iopub.status.idle": "2021-01-16T03:00:21.863122Z",
     "shell.execute_reply": "2021-01-16T03:00:21.862361Z",
     "shell.execute_reply.started": "2021-01-16T03:00:21.537895Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_log_path = './tf_dir/loss_all_layer3_fc_speed_2'\n",
    "loss_summary_writer = tf.summary.create_file_writer(loss_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:21.864745Z",
     "iopub.status.busy": "2021-01-16T03:00:21.864541Z",
     "iopub.status.idle": "2021-01-16T03:00:22.676693Z",
     "shell.execute_reply": "2021-01-16T03:00:22.675703Z",
     "shell.execute_reply.started": "2021-01-16T03:00:21.864721Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss_speed_m = tf.keras.metrics.Mean('train_loss_speed', dtype=tf.float32)\n",
    "train_acc_m = tf.keras.metrics.Mean('train_acc', dtype=tf.float32)\n",
    "dev_loss_speed_m = tf.keras.metrics.Mean('dev_loss_speed', dtype=tf.float32)\n",
    "dev_acc_m = tf.keras.metrics.Mean('dev_acc', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:22.678024Z",
     "iopub.status.busy": "2021-01-16T03:00:22.677827Z",
     "iopub.status.idle": "2021-01-16T03:00:38.635463Z",
     "shell.execute_reply": "2021-01-16T03:00:38.634497Z",
     "shell.execute_reply.started": "2021-01-16T03:00:22.678000Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.load('/openbayes/input/input0/data_all.npy')\n",
    "# y = np.load('/openbayes/input/input1/label_time.npy')\n",
    "y_class = np.load('/openbayes/input/input2/label_class.npy')\n",
    "y_class_oh = tf.one_hot(y_class.astype(np.int32),depth=8,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:38.636904Z",
     "iopub.status.busy": "2021-01-16T03:00:38.636673Z",
     "iopub.status.idle": "2021-01-16T03:00:38.693567Z",
     "shell.execute_reply": "2021-01-16T03:00:38.692783Z",
     "shell.execute_reply.started": "2021-01-16T03:00:38.636875Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=4e-5)\n",
    "epochs = 10000\n",
    "loss_speed_p = 1000\n",
    "model = FcCvModelReFCSpeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:38.694920Z",
     "iopub.status.busy": "2021-01-16T03:00:38.694699Z",
     "iopub.status.idle": "2021-01-16T03:00:38.806313Z",
     "shell.execute_reply": "2021-01-16T03:00:38.805586Z",
     "shell.execute_reply.started": "2021-01-16T03:00:38.694893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2ffa987ba8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights_fc_speed/weights_'+str(5801))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-16T03:00:38.807605Z",
     "iopub.status.busy": "2021-01-16T03:00:38.807383Z",
     "iopub.status.idle": "2021-01-16T03:00:38.823060Z",
     "shell.execute_reply": "2021-01-16T03:00:38.822162Z",
     "shell.execute_reply.started": "2021-01-16T03:00:38.807576Z"
    }
   },
   "outputs": [],
   "source": [
    "# y = y * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:49:03.615257Z",
     "iopub.status.busy": "2021-01-17T08:49:03.614746Z"
    }
   },
   "outputs": [],
   "source": [
    "for e in range(epochs,epochs+10000):\n",
    "    for i in range(12):\n",
    "#         for step in range(2):[step*256:(step+1)*256]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_speed_pred = model(x[i*512:(i+1)*512],training=True)\n",
    "            train_loss_speed = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_speed_pred, y_class_oh[i*512:(i+1)*512]))\n",
    "        grads = tape.gradient(train_loss_speed, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss_speed_m(train_loss_speed)\n",
    "        y_speed_pred_ = tf.argmax(y_speed_pred,axis=-1)\n",
    "        train_acc = tf.reduce_mean(tf.cast(y_speed_pred_ == tf.constant(y_class[i*512:(i+1)*512],dtype=tf.int64),dtype=tf.float32))\n",
    "        train_acc_m(train_acc)\n",
    "    with loss_summary_writer.as_default():\n",
    "        tf.summary.scalar('train_loss_speed', train_loss_speed_m.result(), step=e)\n",
    "        tf.summary.scalar('train_acc', train_acc_m.result(), step=e)\n",
    "    train_loss_speed_m.reset_states()\n",
    "    train_acc_m.reset_states()\n",
    "    if e%100 == 0:\n",
    "        for i in range(4):\n",
    "            y_dev_speed_pred = model(x[(i+12)*512:(i+16)*512],training=False)\n",
    "            dev_loss_speed = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_dev_speed_pred, y_class_oh[(i+12)*512:(i+16)*512]))\n",
    "            dev_loss_speed_m(dev_loss_speed)\n",
    "            y_dev_speed_pred_ = tf.argmax(y_dev_speed_pred,axis=-1)\n",
    "            dev_acc = tf.reduce_mean(tf.cast(y_dev_speed_pred_ == tf.constant(y_class[(i+12)*512:(i+16)*512],dtype=tf.int64),dtype=tf.float32))\n",
    "            dev_acc_m(dev_acc)\n",
    "        with loss_summary_writer.as_default():\n",
    "            tf.summary.scalar('dev_loss_speed', dev_loss_speed_m.result(), step=e)\n",
    "            tf.summary.scalar('dev_acc', dev_acc_m.result(), step=e)\n",
    "        dev_loss_speed_m.reset_states()\n",
    "        dev_acc_m.reset_states()\n",
    "        model.save_weights('weights_fc_speed_2/weights_'+str(e+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
