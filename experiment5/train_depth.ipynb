{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:09.974314Z",
     "iopub.status.busy": "2021-01-17T08:44:09.973853Z",
     "iopub.status.idle": "2021-01-17T08:44:12.429575Z",
     "shell.execute_reply": "2021-01-17T08:44:12.428862Z",
     "shell.execute_reply.started": "2021-01-17T08:44:09.974261Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import optimizers,Sequential, layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:12.431183Z",
     "iopub.status.busy": "2021-01-17T08:44:12.430989Z",
     "iopub.status.idle": "2021-01-17T08:44:12.448749Z",
     "shell.execute_reply": "2021-01-17T08:44:12.448075Z",
     "shell.execute_reply.started": "2021-01-17T08:44:12.431158Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一维卷积后，再全连接获得三层预测\n",
    "class FcCvModelReFCDepth(Model):\n",
    "    def __init__(self):\n",
    "        super(FcCvModelReFCDepth, self).__init__()\n",
    "        # 多层全链接的第一层，每一道数据的提取\n",
    "        # 实际上是在每道上的滑动全连接\n",
    "        self.fcOne = Sequential([\n",
    "            layers.Dense(64),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dense(8),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        # 1.减少过拟合的情况\n",
    "        # 2.模拟道数据的丢失\n",
    "        self.dropout_fc = layers.Dropout(0.3)\n",
    "\n",
    "        # 1维卷积综合提取特征\n",
    "        self.conv1 = Sequential([\n",
    "            layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c1 = layers.Dropout(0.3)\n",
    "        self.conv2 = Sequential([\n",
    "            layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c2 = layers.Dropout(0.3)\n",
    "        # 1维卷积综合提取特征+降维\n",
    "        self.conv3 = Sequential([\n",
    "            layers.Conv1D(filters=128, kernel_size=4, strides=4, padding='valid'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c3 = layers.Dropout(0.3)\n",
    "        # 1维卷积综合提取特征\n",
    "        self.conv4 = Sequential([\n",
    "            layers.Conv1D(filters=128, kernel_size=7, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c4 = layers.Dropout(0.3)\n",
    "        self.conv5 = Sequential([\n",
    "            layers.Conv1D(filters=128, kernel_size=7, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        self.dropout_c5 = layers.Dropout(0.3)\n",
    "        # 1维卷积综合提取特征+降维\n",
    "        self.conv6 = Sequential([\n",
    "            layers.Conv1D(filters=256, kernel_size=4, strides=4, padding='valid'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "#         self.dropout_c6 = layers.Dropout(0.3)\n",
    "        # 最终作出的预测\n",
    "        # 深度预测\n",
    "        self.fcThree_d1 = Sequential([\n",
    "            layers.Dense(256)\n",
    "        ])\n",
    "\n",
    "        self.fcThree_d2 = Sequential([\n",
    "            layers.Dense(256)\n",
    "        ])\n",
    "\n",
    "        self.fcThree_d3 = Sequential([\n",
    "            layers.Dense(256)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        batchSize, dao_num, data_num = inputs.shape\n",
    "        x = tf.reshape(inputs, (-1, 1024))\n",
    "        x = self.fcOne(x)\n",
    "        x = tf.reshape(x, (batchSize, dao_num, -1))\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])\n",
    "        x = self.dropout_fc(x, training=training)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout_c1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout_c2(x, training=training)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout_c3(x, training=training)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout_c4(x, training=training)\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout_c5(x, training=training)\n",
    "        x = self.conv6(x)\n",
    "#         x = self.dropout_c6(x, training=training)\n",
    "        x = tf.reshape(x, (batchSize, -1))\n",
    "        x_d1 = self.fcThree_d1(x)\n",
    "        x_d2 = self.fcThree_d2(x)\n",
    "        x_d3 = self.fcThree_d3(x)\n",
    "        x_d = tf.concat([tf.expand_dims(x_d1,axis=1),tf.expand_dims(x_d2,axis=1),tf.expand_dims(x_d3,axis=1)],axis=1)\n",
    "        return x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:12.450130Z",
     "iopub.status.busy": "2021-01-17T08:44:12.449947Z",
     "iopub.status.idle": "2021-01-17T08:44:12.852365Z",
     "shell.execute_reply": "2021-01-17T08:44:12.850813Z",
     "shell.execute_reply.started": "2021-01-17T08:44:12.450107Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_log_path = './tf_dir/loss_all_layer3_fc_depth_4'\n",
    "loss_summary_writer = tf.summary.create_file_writer(loss_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:12.854760Z",
     "iopub.status.busy": "2021-01-17T08:44:12.854509Z",
     "iopub.status.idle": "2021-01-17T08:44:13.735510Z",
     "shell.execute_reply": "2021-01-17T08:44:13.734828Z",
     "shell.execute_reply.started": "2021-01-17T08:44:12.854729Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss_depth_m = tf.keras.metrics.Mean('train_loss_depth', dtype=tf.float32)\n",
    "dev_loss_depth_m = tf.keras.metrics.Mean('dev_loss_depth', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:13.736706Z",
     "iopub.status.busy": "2021-01-17T08:44:13.736506Z",
     "iopub.status.idle": "2021-01-17T08:44:13.739442Z",
     "shell.execute_reply": "2021-01-17T08:44:13.738884Z",
     "shell.execute_reply.started": "2021-01-17T08:44:13.736680Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.load_weights('weights/weights_'+str(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:13.740447Z",
     "iopub.status.busy": "2021-01-17T08:44:13.740259Z",
     "iopub.status.idle": "2021-01-17T08:44:31.648278Z",
     "shell.execute_reply": "2021-01-17T08:44:31.647345Z",
     "shell.execute_reply.started": "2021-01-17T08:44:13.740425Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.load('/openbayes/input/input0/data_all.npy')\n",
    "# y = np.load('/openbayes/input/input1/label_time.npy')\n",
    "# y_class = np.load('/openbayes/input/input2/label_class.npy')\n",
    "y_depth = np.load('/openbayes/input/input2/label_depth.npy')*4\n",
    "# y_class_oh = tf.one_hot(y_class.astype(np.int32),depth=8,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:31.649821Z",
     "iopub.status.busy": "2021-01-17T08:44:31.649588Z",
     "iopub.status.idle": "2021-01-17T08:44:31.699445Z",
     "shell.execute_reply": "2021-01-17T08:44:31.698385Z",
     "shell.execute_reply.started": "2021-01-17T08:44:31.649792Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=1e-4)\n",
    "epochs = 10000\n",
    "model = FcCvModelReFCDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:31.701523Z",
     "iopub.status.busy": "2021-01-17T08:44:31.701295Z",
     "iopub.status.idle": "2021-01-17T08:44:31.711226Z",
     "shell.execute_reply": "2021-01-17T08:44:31.710621Z",
     "shell.execute_reply.started": "2021-01-17T08:44:31.701494Z"
    }
   },
   "outputs": [],
   "source": [
    "# y = y * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T08:44:42.487300Z",
     "iopub.status.busy": "2021-01-17T08:44:42.486819Z"
    }
   },
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(12):\n",
    "#         for step in range(2):[step*256:(step+1)*256]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_depth_pred = model(x[i*512:(i+1)*512],training=True)\n",
    "            train_loss_depth = tf.reduce_mean(tf.losses.MSE(y_depth_pred, y_depth[i*512:(i+1)*512]))\n",
    "        grads = tape.gradient(train_loss_depth, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss_depth_m(train_loss_depth)\n",
    "    with loss_summary_writer.as_default():\n",
    "        tf.summary.scalar('train_loss_depth', train_loss_depth_m.result(), step=e)\n",
    "    train_loss_depth_m.reset_states()\n",
    "    if e%100 == 0:\n",
    "        for i in range(4):\n",
    "            y_dev_depth_pred = model(x[(i+12)*512:(i+16)*512],training=False)\n",
    "            dev_loss_depth = tf.reduce_mean(tf.losses.MSE(y_dev_depth_pred, y_depth[(i+12)*512:(i+16)*512]))\n",
    "            dev_loss_depth_m(dev_loss_depth)\n",
    "        with loss_summary_writer.as_default():\n",
    "            tf.summary.scalar('dev_loss_depth', dev_loss_depth_m.result(), step=e)\n",
    "        dev_loss_depth_m.reset_states()\n",
    "        model.save_weights('weights_fc_depth_4/weights_'+str(e+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
